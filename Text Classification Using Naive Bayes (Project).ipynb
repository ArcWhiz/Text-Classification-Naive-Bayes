{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making test vocabulary and finding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "x = []   #x is the list where 1st element is name of document and second element is text in document\n",
    "y = []   # y is the category\n",
    "for category in os.listdir(\"20_newsgroups\"):\n",
    "    for document in os.listdir(\"20_newsgroups/\" + category):\n",
    "        with open(\"20_newsgroups/\" + category + '/' + document, \"r\") as f:\n",
    "            x.append((document,f.read()))\n",
    "            y.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making test vocabulary\n",
    "words = {}\n",
    "stop_words = {\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \n",
    "              \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \n",
    "              \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \n",
    "              \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \n",
    "              \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \n",
    "              \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\",\n",
    "              \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\",\n",
    "              \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\",\n",
    "              \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\",\n",
    "              \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\",\n",
    "              \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\",\n",
    "              \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\",\n",
    "              \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\",\n",
    "              \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\",\n",
    "              \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\",\n",
    "              \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\",\n",
    "              \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\",\n",
    "              \"how\", \"further\", \"was\", \"here\", \"than\"}\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for word in x[i][1].split():\n",
    "        if word.lower() in stop_words:\n",
    "            continue\n",
    "        elif word.lower() in words:\n",
    "            words[word.lower()] += 1\n",
    "        else:\n",
    "            words[word.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get k most frequent words from vocabulary \n",
    "def get_top_k(words, k):\n",
    "    freq = np.array(list(words.values()))\n",
    "    freq = np.sort(freq, axis = -1, kind = \"quicksort\", order = None)[::-1]\n",
    "    new_words = {key : val for key, val in words.items() if val >= freq[k - 1]}\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping top frequnecy words to an index value\n",
    "top_k_words = get_top_k(words, 1000)\n",
    "i = 0\n",
    "for word in top_k_words.keys():\n",
    "    top_k_words[word] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making final 2D array with top words as features to begin algorithm implementation\n",
    "rows, cols = (len(x), len(top_k_words)) \n",
    "final_x = [[0 for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for word in x[i][1].split():\n",
    "        if word.lower() in top_k_words:\n",
    "            final_x[i][top_k_words[word.lower()]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting lists to numpy arrays for easier manipulation\n",
    "final_x = np.array(final_x)\n",
    "final_y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(final_x, final_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the count dictionary\n",
    "def get_count_dict(X_train, Y_train):\n",
    "    count = {}\n",
    "    target_names = set(Y_train)\n",
    "    \n",
    "    for target in target_names:\n",
    "        X_subset = X_train[Y_train == target]\n",
    "        count[target] = {}\n",
    "        count[target][\"total\"] = 0\n",
    "        column_sum = np.sum(X_subset, axis = 0)\n",
    "        for i in range(X_train.shape[1]):\n",
    "            count[target][i] = column_sum[i]\n",
    "            count[target][\"total\"] += column_sum[i]\n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = get_count_dict(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm implementation for a single document\n",
    "def predict_for_one(X_test_row):\n",
    "    \n",
    "    target_names = set(Y_train)\n",
    "    max_probability = 0\n",
    "    first_run = True\n",
    "    predicted_target = \"\"\n",
    "    \n",
    "    for target in target_names:\n",
    "        \n",
    "        target_probability = 0\n",
    "        \n",
    "        for i in range(X_train.shape[1]):\n",
    "            num = count[target][i] + 1\n",
    "            den = count[target][\"total\"] + X_train.shape[1]\n",
    "            target_probability += (X_test_row[i] * np.log2(num / den))\n",
    "            \n",
    "        if (target_probability > max_probability) or (first_run):\n",
    "            max_probability = target_probability\n",
    "            predicted_target = target\n",
    "            first_run = False\n",
    "            \n",
    "    return predicted_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm implementation for all test documents\n",
    "def multi_naive_bayes():\n",
    "    Y_pred = []\n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        Y_pred.append(predict_for_one(X_test[i]))\n",
    "        \n",
    "    Y_pred = np.array(Y_pred)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = multi_naive_bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inbuilt implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_inbuilt = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between the two implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.68      0.71      0.70       263\n",
      "           comp.graphics       0.72      0.71      0.71       246\n",
      " comp.os.ms-windows.misc       0.84      0.74      0.79       237\n",
      "comp.sys.ibm.pc.hardware       0.84      0.80      0.82       256\n",
      "   comp.sys.mac.hardware       0.80      0.91      0.85       235\n",
      "          comp.windows.x       0.90      0.68      0.78       246\n",
      "            misc.forsale       0.58      0.91      0.71       256\n",
      "               rec.autos       0.68      0.81      0.74       244\n",
      "         rec.motorcycles       0.73      0.89      0.80       236\n",
      "      rec.sport.baseball       0.75      0.91      0.82       249\n",
      "        rec.sport.hockey       0.94      0.62      0.75       256\n",
      "               sci.crypt       0.92      0.82      0.87       248\n",
      "         sci.electronics       0.72      0.82      0.76       262\n",
      "                 sci.med       0.71      0.79      0.75       230\n",
      "               sci.space       0.78      0.79      0.78       268\n",
      "  soc.religion.christian       0.93      0.98      0.96       251\n",
      "      talk.politics.guns       0.67      0.79      0.73       234\n",
      "   talk.politics.mideast       0.88      0.69      0.78       272\n",
      "      talk.politics.misc       0.66      0.36      0.47       234\n",
      "      talk.religion.misc       0.56      0.39      0.46       277\n",
      "\n",
      "                accuracy                           0.76      5000\n",
      "               macro avg       0.76      0.76      0.75      5000\n",
      "            weighted avg       0.76      0.76      0.75      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Classification report for own implementation\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.68      0.71      0.70       263\n",
      "           comp.graphics       0.72      0.71      0.71       246\n",
      " comp.os.ms-windows.misc       0.84      0.75      0.79       237\n",
      "comp.sys.ibm.pc.hardware       0.84      0.80      0.82       256\n",
      "   comp.sys.mac.hardware       0.80      0.91      0.85       235\n",
      "          comp.windows.x       0.90      0.68      0.78       246\n",
      "            misc.forsale       0.59      0.91      0.71       256\n",
      "               rec.autos       0.68      0.81      0.74       244\n",
      "         rec.motorcycles       0.73      0.89      0.80       236\n",
      "      rec.sport.baseball       0.76      0.91      0.83       249\n",
      "        rec.sport.hockey       0.95      0.62      0.75       256\n",
      "               sci.crypt       0.92      0.82      0.87       248\n",
      "         sci.electronics       0.72      0.82      0.77       262\n",
      "                 sci.med       0.71      0.79      0.75       230\n",
      "               sci.space       0.78      0.78      0.78       268\n",
      "  soc.religion.christian       0.93      0.98      0.96       251\n",
      "      talk.politics.guns       0.67      0.79      0.73       234\n",
      "   talk.politics.mideast       0.88      0.69      0.78       272\n",
      "      talk.politics.misc       0.66      0.37      0.47       234\n",
      "      talk.religion.misc       0.56      0.39      0.46       277\n",
      "\n",
      "                accuracy                           0.76      5000\n",
      "               macro avg       0.77      0.76      0.75      5000\n",
      "            weighted avg       0.76      0.76      0.75      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for inbuilt implementation\n",
    "print(classification_report(Y_test, Y_pred_inbuilt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
